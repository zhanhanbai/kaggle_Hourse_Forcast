{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 2, 100)\n",
    "y = np.random.randint(0, 2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.astype(np.float32)\n",
    "y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(x, y, average=\"micro\")\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(x, y, average=\"micro\")\n",
    "f1 = f1_score(x, y, average=\"micro\")\n",
    "(recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc \n",
    "import numpy as np\n",
    "y = np.array([1, 1, 2, 2])  \n",
    "scores = np.array([0.1, 0.4, 0.35, 0.8])  \n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# 生成示例数据\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "y = np.array([0, 1, 0, 1, 0, 1])\n",
    "\n",
    "# 创建 KFold 对象，将数据分成 3 个折\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# 迭代每个折\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # 在训练集上训练模型（这里做一个简单的打印示例）\n",
    "    print(\"Train Set:\")\n",
    "    print(\"X_train:\", X_train)\n",
    "    print(\"y_train:\", y_train)\n",
    "    \n",
    "    # 在测试集上测试模型（这里做一个简单的打印示例）\n",
    "    print(\"Test Set:\")\n",
    "    print(\"X_test:\", X_test)\n",
    "    print(\"y_test:\", y_test)\n",
    "    \n",
    "    print(\"=======================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "train = pd.read_csv(\"/Users/zhenhan/Desktop/深度学习/data/房屋数据（2011.8-2015.6）.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "for col in train.columns:\n",
    "    stats.append([\n",
    "        col,\n",
    "        train[col].nunique(),\n",
    "        train[col].isna().sum() * 100 / train.shape[0],\n",
    "        train[col].value_counts(normalize=True, dropna=False).values[0] * 100,\n",
    "        train[col].dtype\n",
    "    ])\n",
    "stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values','Missing_values_percent', 'biggest values', 'type'])\n",
    "stats_df.sort_values('Missing_values_percent', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[\"type\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = train.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "missing.sort_values(inplace=True)\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"price_doc\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sen \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sen.displot(train[\"price_doc\"],  bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sen \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sen.displot(np.log(train[\"price_doc\"]),  bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = train.select_dtypes(include=['float64', 'int64'])\n",
    "df_num = df_num[df_num.columns.tolist()[::-1][0:4]]\n",
    "df_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = train.corr()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20,9))\n",
    "sen.heatmap(corrmat, vmax=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat[\"price_doc\"].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat[\"price_doc\"].sort_values(ascending=False)[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_num = train.select_dtypes(['object'])\n",
    "df_num = df_num[df_num.columns.to_list()[1:5]]\n",
    "for col in df_num.columns:\n",
    "    value_counts = df_num[col].value_counts()\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x=value_counts.index, y=value_counts.values, palette='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /var/folders/_n/4rn71s9d0kbcfsx7vgnkblrc0000gn/T/matplotlib-9bz6erd9 because the default path (/Users/zhenhan/.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/Users/zhenhan/Desktop/深度学习/data/房屋数据（2011.8-2015.6）.csv\")\n",
    "train_isna = []\n",
    "for col in train.columns:\n",
    "    if train[col].isna().sum() / train.shape[0] > 0.4:\n",
    "        train_isna.append(col)\n",
    "df = train.drop(train_isna, axis=1)\n",
    "df_num = df.select_dtypes(include=['int64', 'float64'])\n",
    "df_obj = df.select_dtypes(include=['object'])\n",
    "df_num_means = df_num.median()\n",
    "df_num = df_num.fillna(df_num_means)\n",
    "df_num.isna().sum().value_counts()\n",
    "df_num = df_num.iloc[:,1:]\n",
    "df_obj = df_obj.iloc[:,1:]\n",
    "df_obj_map = {\"poor\":0, \"no data\":2, \"satisfactory\":2, \"good\":3, \"excellent\":4}\n",
    "df_obj[\"ecology\"] = df_obj[\"ecology\"].map(df_obj_map)\n",
    "label_encoder = LabelEncoder()\n",
    "df_obj['sub_area'] = label_encoder.fit_transform(df_obj['sub_area'])\n",
    "df_obj = pd.get_dummies(df_obj, columns=df_obj.columns[2:14], prefix=df_obj.columns[2:14])\n",
    "df_obj = pd.get_dummies(df_obj, columns=['product_type'], prefix=['product_type'])\n",
    "target_column = df_num.columns[-1]\n",
    "for col in df_num.columns:\n",
    "    correlation = df[col].corr(df[target_column])\n",
    "    if abs(correlation) <= 0.05:\n",
    "        df_num = df_num.drop(col, axis=1)\n",
    "data = pd.concat([df_num, df_obj], axis=1)\n",
    "X = data.drop('price_doc', axis=1)\n",
    "y = data['price_doc']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = XGBRegressor(n_estimators=500, max_depth=8, learning_rate=0.01, random_state=42)\n",
    "model_1.fit(X_train, y_train)\n",
    "\n",
    "y_pre = model_1.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pre))\n",
    "print(rmse)\n",
    "r2 = r2_score(y_test, y_pre)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = XGBRegressor()\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': range(100, 1001, 100),\n",
    "    'max_depth': range(3, 16, 3),\n",
    "    'learning_rate': np.linspace(0.001, 1, 10),\n",
    "    'subsample': np.linspace(0.5, 1, 10),\n",
    "    'colsample_bytree': np.linspace(0.3, 1, 10),\n",
    "    'min_child_weight': range(1, 6, 1)\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(model_1,\n",
    "                        param_dist,\n",
    "                        cv=5,\n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        n_iter=100,\n",
    "                        n_jobs=8)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "best_estimator = grid.best_estimator_\n",
    "print(best_estimator)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = LGBMRegressor()\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': range(100, 1001, 100),\n",
    "    'max_depth': range(3, 16, 3),\n",
    "    'learning_rate': np.linspace(0.001, 1, 10),\n",
    "    'subsample': np.linspace(0.5, 1, 10),\n",
    "    'colsample_bytree': np.linspace(0.3, 1, 10),\n",
    "    'min_child_samples': range(1, 6, 1)  # 修改超参数名称\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "    model_2,\n",
    "    param_distributions=param_dist,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_iter=100,\n",
    "    n_jobs=8\n",
    ")\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "best_estimator = grid.best_estimator_\n",
    "best_params = grid.best_params_\n",
    "best_score = grid.best_score_\n",
    "\n",
    "print(best_estimator)\n",
    "print(best_params)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(1024, 1024)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(1024, 256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc5 = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outs = self.fc1(x).clamp(min=0) \n",
    "        outs = self.relu1(outs)\n",
    "        \n",
    "        outs = self.fc2(outs).clamp(min=0) \n",
    "        outs = self.relu2(outs)\n",
    "        \n",
    "        outs = self.fc3(outs).clamp(min=0) \n",
    "        outs = self.relu3(outs)\n",
    "\n",
    "        outs = self.fc4(outs).clamp(min=0) \n",
    "        outs = self.relu4(outs)\n",
    "\n",
    "        outs = self.fc5(outs)\n",
    "        \n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train.values).float(), torch.tensor(y_train.values).float()), batch_size=32, shuffle=True)\n",
    "val_laoder = DataLoader(TensorDataset(torch.tensor(X_test.values).float(), torch.tensor(y_test.values).float()), batch_size=32, shuffle=False)\n",
    "\n",
    "model_3 = MLP_Net()\n",
    "\n",
    "loss_func = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.AdamW(params=model_3.parameters(), lr = 0.01)\n",
    "\n",
    "def metric_func(y_pred, y_true):\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "epochs = 20\n",
    "log_step_freq = 300\n",
    "dishistory = pd.DataFrame(columns= [\"epochs\", \"train_loss\", \"val_loss\", \"Train_rmse\", \"Train_R^2\",\"val_rmse\", \"val_R^2\"])\n",
    "print(\"Strat Training\")\n",
    "nowtime = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"==========\"*8 + \"%s\"%nowtime)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model_3.train()\n",
    "    loss_sum = 0.\n",
    "    rmse_sum = 0.\n",
    "    r2_sum = 0.\n",
    "    step = 1\n",
    "\n",
    "    for step, (feature, target) in enumerate(train_loader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model_3(feature)\n",
    "        loss = loss_func(predictions, target.view(-1, 1))\n",
    "        rmse, r2  = metric_func(predictions, target.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "        rmse_sum += rmse.item()\n",
    "        r2_sum += r2.item()\n",
    "\n",
    "        if step%log_step_freq == 0:\n",
    "            print(\"[step = %d] loss: %d, rmse: %d, r2: %.3f\" % (step, loss_sum/step, rmse_sum/step, r2_sum/step))\n",
    "\n",
    "    model_3.eval()\n",
    "    val_loss_sum = 0.\n",
    "    val_rmse_sum = 0.\n",
    "    val_r2_sum = 0.\n",
    "    val_step = 1\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state_dict = None\n",
    "\n",
    "    for val_step, (feature, target) in enumerate(val_laoder, 1):\n",
    "        predictions = model_3(feature)\n",
    "        val_loss = loss_func(predictions, target.view(-1,1))\n",
    "        val_rmse, val_r2 = metric_func(predictions, target)\n",
    "\n",
    "        val_loss_sum += val_loss.item()\n",
    "        val_rmse_sum += val_rmse.item()\n",
    "        val_r2_sum += val_r2.item()\n",
    "\n",
    "        # if val_step%log_step_freq == 0:\n",
    "        #     print(\"[step = %d] loss: %d, rmse: %d, r2: %.3f\") % (step, val_loss_sum/step, val_rmse_sum/step, val_r2_sum/step)\n",
    "    \n",
    "\n",
    "    info = (epoch, loss_sum/step, val_loss_sum/val_step, rmse_sum/step, r2_sum/step, val_rmse_sum/val_step, val_r2/val_step)\n",
    "    dishistory.loc[epoch-1] = info\n",
    "    if (val_loss_sum / val_step) < best_val_loss:\n",
    "        best_val_loss = val_loss_sum / val_step\n",
    "        best_model_state_dict = model_3.state_dict()\n",
    "    \n",
    "    torch.save(best_model_state_dict, 'best_model.pth')\n",
    "\n",
    "    print(\"[EPOCH = %d] trian_loss = %d, val_loss = %d, trian_rmse =  %d, train_r2 = %.3f, val_rmse = %d, val_r2 = %.3f\" % info)\n",
    "    nowtime = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = MLP_Net()\n",
    "model_3.load_state_dict(torch.load('./best_model.pth'))\n",
    "model_3.eval()\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, (feature, target) in enumerate(val_laoder):\n",
    "        output = model_3(feature)\n",
    "        predictions.append(output.cpu().numpy())\n",
    "        targets.append(target.cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions)\n",
    "targets = np.concatenate(targets)\n",
    "# print(predictions)\n",
    "rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
    "r2 = r2_score(targets, predictions)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R^2:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dishistory\n",
    "x = df[\"epochs\"]\n",
    "y_columns = [\"train_loss\", \"val_loss\", \"Train_rmse\", \"Train_R^2\", \"val_rmse\", \"val_R^2\"]\n",
    "y = df[y_columns]\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for column in y_columns:\n",
    "    plt.plot(x, df[column], marker='o', label=column)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.title(\"Training and Validation Metrics\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 初始化不同的回归器\n",
    "# reg1 = SVR()\n",
    "# reg2 = RandomForestRegressor()\n",
    "reg3 = GradientBoostingRegressor()\n",
    "reg4 = XGBRegressor()\n",
    "reg5 = LGBMRegressor()\n",
    "reg6 = CatBoostRegressor()\n",
    "\n",
    "# 训练并预测\n",
    "for reg in [reg3, reg4, reg5, reg6]:\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    mse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{reg.__class__.__name__} MSE: {mse:.4f}\")\n",
    "    print(f\"{reg.__class__.__name__} R2: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# 初始化不同的回归器\n",
    "# reg1 = SVR()\n",
    "# reg2 = RandomForestRegressor()\n",
    "reg3 = GradientBoostingRegressor()\n",
    "reg4 = XGBRegressor()\n",
    "reg5 = LGBMRegressor()\n",
    "reg6 = CatBoostRegressor()\n",
    "\n",
    "# 将回归器放入列表中\n",
    "regressors = [\n",
    "            #   ('SVR', reg1),\n",
    "            #   ('Random Forest', reg2),\n",
    "              ('Gradient Boosting', reg3),\n",
    "              ('XGBoost', reg4),\n",
    "              ('LGBoost', reg5),\n",
    "              ('CATBoost', reg6)]\n",
    "\n",
    "# 初始化投票回归器\n",
    "voting_reg = VotingRegressor(estimators=regressors)\n",
    "\n",
    "# 训练投票回归器\n",
    "voting_reg.fit(X_train, y_train)\n",
    "y_pred = voting_reg.predict(X_test)\n",
    "mse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END learning_rate=0.02, max_depth=10, n_estimators=900, subsample=0.5; total time=   9.4s\n",
      "[CV] END learning_rate=0.02, max_depth=10, n_estimators=900, subsample=0.5; total time=   9.5s\n",
      "[CV] END learning_rate=0.02, max_depth=10, n_estimators=900, subsample=0.5; total time=   9.6s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法执行代码，已释放会话。请尝试重新启动内核。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "lgb_reg = lgb.LGBMRegressor()\n",
    "# lgb_reg = XGBRegressor()\n",
    "# lgb_reg = CatBoostRegressor()\n",
    "# 定义要调优的参数和对应的候选值\n",
    "param_grid = {\n",
    "    'learning_rate': [0.02],\n",
    "    'max_depth': [10],\n",
    "    'n_estimators': [900],\n",
    "    'subsample': [0.5]\n",
    "}\n",
    "\n",
    "# 初始化网格搜索对象\n",
    "grid_search = GridSearchCV(estimator=lgb_reg, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=2, n_jobs=8)\n",
    "\n",
    "# 在训练集上进行网格搜索调参\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 输出最优参数和最优模型\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best negative mean squared error found: \", grid_search.best_score_)\n",
    "\n",
    "# 使用最优参数的模型进行预测\n",
    "best_lgb_reg = grid_search.best_estimator_\n",
    "y_pred = best_lgb_reg.predict(X_test)\n",
    "\n",
    "# 计算均方误差\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Mean Squared Error on test set: \", rmse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R2 Error on test set: \", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
